{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T03:01:02.017051Z",
     "start_time": "2020-03-17T03:00:47.664880Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 36 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import init\n",
    "import torch.utils.data as Data\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "N_SAMPLES=200\n",
    "BATCH_SIZE=64\n",
    "EPOCH=12\n",
    "LR=0.03\n",
    "N_HIDDEN=8\n",
    "ACTIVATION=torch.tanh\n",
    "B_INIT=-0.2       # use a bad bias constant initializer\n",
    "\n",
    "#training data \n",
    "x=np.linspace(-7,10,N_SAMPLES)[:,np.newaxis]\n",
    "noise=np.random.normal(0,2,x.shape) # 02之间\n",
    "y=np.square(x)-5+noise\n",
    "\n",
    "#test data\n",
    "test_x=np.linspace(-7,10,200)[:,np.newaxis]\n",
    "noise=np.random.normal(0,2,test_x.shape)\n",
    "test_y=np.square(test_x)-5+noise\n",
    "\n",
    "train_x,train_y=torch.from_numpy(x).float(),torch.from_numpy(y).float()\n",
    "test_x=torch.from_numpy(test_x).float()\n",
    "test_y=torch.from_numpy(test_y).float()\n",
    "\n",
    "train_dataset=Data.TensorDataset(train_x,train_y)\n",
    "train_loader=Data.DataLoader(dataset=train_dataset,batch_size=BATCH_SIZE,shuffle=True,num_workers=2)\n",
    "\n",
    "plt.scatter(train_x.numpy(), train_y.numpy(), c='#FF9359', s=50, alpha=0.2, label='train')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,batch_normalization=False):\n",
    "        super(Net,self).__init__()\n",
    "        self.do_bn=batch_normalization\n",
    "        self.fcs=[]# full connection\n",
    "        self.bns=[]# batch normalization\n",
    "        self.bn_input=nn.BatchNorm1d(1,momentum=0.5)\n",
    "        \n",
    "        for i in range(N_HIDDEN): #定义每一层，从第一层开始\n",
    "            input_size=1 if i==0 else 10\n",
    "            fc=nn.Linear(input_size,10)\n",
    "            setattr(self,'fc%i'%i,fc)   #add fc to Net ,类似self.fc=fc\n",
    "            self._set_init(fc)\n",
    "            self.fcs.append(fc)\n",
    "            if self.do_bn:\n",
    "                bn=nn.BatchNorm1d(10,momentum=0.5)\n",
    "                setattr(self,'bn%i'%i,bn)\n",
    "                self.bns.append(bn)\n",
    "            \n",
    "        self.predict=nn.Linear(10,1)\n",
    "        self._set_init(self.predict)\n",
    "    \n",
    "    def _set_init(self,layer):  # parameters initialization\n",
    "        init.normal_(layer.weight,mean=0,std=1)\n",
    "        init.constant_(layer.bias,B_INIT)         #对某一层的参数进行初始化\n",
    "        \n",
    "    def forward(self,x):\n",
    "        pre_activation=[x]  #激励之前的x\n",
    "        if self.do_bn:self.bn_input(x)  #输入bn之后的数据\n",
    "        layer_input=[x]   #经过bn，后面层的输入\n",
    "        for i in range(N_HIDDEN):\n",
    "            x=self.fcs[i](x)\n",
    "            pre_activation.append(x)\n",
    "            if self.do_bn:x=self.bns[i](x)\n",
    "            x=ACTIVATION(x)\n",
    "            layer_input.append(x)\n",
    "        out=self.predict(x)\n",
    "        \n",
    "        return out ,layer_input,pre_activation\n",
    "            \n",
    "nets=[Net(batch_normalization=False),Net(batch_normalization=True)]\n",
    "#print(nets)\n",
    "    \n",
    "opts=[torch.optim.Adam(net.parameters(),lr=LR)for net in nets]\n",
    "loss_func=torch.nn.MSELoss()\n",
    "\n",
    "def plot_histogram(l_in, l_in_bn, pre_ac, pre_ac_bn):\n",
    "    for i, (ax_pa, ax_pa_bn, ax, ax_bn) in enumerate(zip(axs[0, :], axs[1, :], axs[2, :], axs[3, :])):\n",
    "        [a.clear() for a in [ax_pa, ax_pa_bn, ax, ax_bn]]\n",
    "        if i == 0:\n",
    "            p_range = (-7, 10);the_range = (-7, 10)\n",
    "        else:\n",
    "            p_range = (-4, 4);the_range = (-1, 1)\n",
    "        ax_pa.set_title('L' + str(i))\n",
    "        ax_pa.hist(pre_ac[i].data.numpy().ravel(), bins=10, range=p_range, color='#FF9359', alpha=0.5);ax_pa_bn.hist(pre_ac_bn[i].data.numpy().ravel(), bins=10, range=p_range, color='#74BCFF', alpha=0.5)\n",
    "        ax.hist(l_in[i].data.numpy().ravel(), bins=10, range=the_range, color='#FF9359');ax_bn.hist(l_in_bn[i].data.numpy().ravel(), bins=10, range=the_range, color='#74BCFF')\n",
    "        for a in [ax_pa, ax, ax_pa_bn, ax_bn]: a.set_yticks(());a.set_xticks(())\n",
    "        ax_pa_bn.set_xticks(p_range);ax_bn.set_xticks(the_range)\n",
    "        axs[0, 0].set_ylabel('PreAct');axs[1, 0].set_ylabel('BN PreAct');axs[2, 0].set_ylabel('Act');axs[3, 0].set_ylabel('BN Act')\n",
    "    plt.pause(0.01)\n",
    "\n",
    "\n",
    "f, axs = plt.subplots(4, N_HIDDEN + 1, figsize=(10, 5))\n",
    "plt.ion()  # something about plotting\n",
    "plt.show()\n",
    "\n",
    "losses=[[],[]]\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    print('Epoch',epoch)\n",
    "    layer_inputs,pre_acts=[],[]\n",
    "    for net, l in zip(nets, losses):\n",
    "        net.eval()  \n",
    "        pred, layer_input, pre_act = net(test_x)\n",
    "        l.append(loss_func(pred, test_y).data.item())\n",
    "        layer_inputs.append(layer_input)\n",
    "        pre_acts.append(pre_act)\n",
    "        net.train()  \n",
    "    plot_histogram(*layer_inputs, *pre_acts)     # plot histogram    \n",
    "    \n",
    "    for step, (b_x, b_y) in enumerate(train_loader):\n",
    "            for net, opt in zip(nets, opts):     # train for each network\n",
    "                pred, _, _ = net(b_x)\n",
    "                loss = loss_func(pred, b_y)\n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step() \n",
    "    \n",
    "plt.ioff()\n",
    "\n",
    "    # plot training loss\n",
    "plt.figure(2)\n",
    "plt.plot(losses[0], c='#FF9359', lw=3, label='Original')\n",
    "plt.plot(losses[1], c='#74BCFF', lw=3, label='Batch Normalization')\n",
    "plt.xlabel('step');plt.ylabel('test loss');plt.ylim((0, 2000));plt.legend(loc='best')\n",
    "\n",
    "    # evaluation\n",
    "    # set net to eval mode to freeze the parameters in batch normalization layers\n",
    "[net.eval() for net in nets]    # set eval mode to fix moving_mean and moving_var\n",
    "preds = [net(test_x)[0] for net in nets]\n",
    "plt.figure(3)\n",
    "plt.plot(test_x.data.numpy(), preds[0].data.numpy(), c='#FF9359', lw=4, label='Original')\n",
    "plt.plot(test_x.data.numpy(), preds[1].data.numpy(), c='#74BCFF', lw=4, label='Batch Normalization')\n",
    "plt.scatter(test_x.data.numpy(), test_y.data.numpy(), c='r', s=50, alpha=0.2, label='train')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropout改善过拟合\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N_SAMPLES=20\n",
    "N_HIDDEN=300\n",
    "\n",
    "#training data\n",
    "x=torch.unsqueeze(torch.linspace(-1,1,N_SAMPLES),1)\n",
    "y=x+0.3*torch.normal(torch.zeros(N_SAMPLES,1),torch.ones(N_SAMPLES,1))#mean std\n",
    "x,y=Variable(x),Variable(y)\n",
    "\n",
    "#test data\n",
    "test_x=torch.unsqueeze(torch.linspace(-1,1,N_SAMPLES),1)\n",
    "test_y=test_x+0.3*torch.normal(torch.zeros(N_SAMPLES,1),torch.ones(N_SAMPLES,1))\n",
    "test_x,test_y=Variable(test_x),Variable(test_y)\n",
    "\n",
    "#show data\n",
    "plt.scatter(x.data.numpy(), y.data.numpy(), c='magenta', s=50, alpha=0.5, label='train')\n",
    "plt.scatter(test_x.data.numpy(), test_y.data.numpy(), c='cyan', s=50, alpha=0.5, label='test')\n",
    "plt.legend(loc='upper left')\n",
    "plt.ylim((-2.5, 2.5))\n",
    "plt.show()\n",
    "\n",
    "net_overfitting=torch.nn.Sequential(\n",
    "    torch.nn.Linear(1,N_HIDDEN),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(N_HIDDEN,N_HIDDEN),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(N_HIDDEN,1)\n",
    ")\n",
    "\n",
    "net_dropped=torch.nn.Sequential(\n",
    "    torch.nn.Linear(1,N_HIDDEN),\n",
    "    torch.nn.Dropout(0.5),   #drop 50% neuron\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(N_HIDDEN,N_HIDDEN),\n",
    "    torch.nn.Dropout(0.5),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(N_HIDDEN,1)\n",
    ")\n",
    "\n",
    "print(net_overfitting)\n",
    "print(net_dropout)\n",
    "\n",
    "optimizer_ofit=torch.optim.Adam(net_overfitting.parameters(),lr=0.01)\n",
    "optimizer_drop=torch.optim.Adam(net_dropped.parameters(),lr=0.01)\n",
    "loss_func=torch.nn.MSELoss()\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "for t in range(500):\n",
    "    pred_ofit=net_overfitting(x)\n",
    "    pred_drop=net_dropped(x)\n",
    "    loss_ofit=loss_func(pred_ofit,y)\n",
    "    loss_drop=loss_func(pred_drop,y)\n",
    "    \n",
    "    optimizer_ofit.zero_grad()\n",
    "    optimizer_drop.zero_grad()\n",
    "    loss_ofit.backward()\n",
    "    loss_drop.backward()\n",
    "    optimizer_ofit.step()\n",
    "    optimizer_drop.step()\n",
    "    \n",
    "    if t%10==0:\n",
    "        net_overfitting.eval()\n",
    "        net_dropped.eval()     #在测试的时候屏蔽掉drop的部分。避免参数不同\n",
    "        \n",
    "        plt.cla()\n",
    "        test_pred_ofit=net_overfitting(test_x)\n",
    "        test_pred_drop=net_dropped(test_x)\n",
    "        plt.scatter(x.data.numpy(), y.data.numpy(), c='magenta', s=50, alpha=0.3, label='train')\n",
    "        plt.scatter(test_x.data.numpy(), test_y.data.numpy(), c='cyan', s=50, alpha=0.3, label='test')\n",
    "        plt.plot(test_x.data.numpy(), test_pred_ofit.data.numpy(), 'r-', lw=3, label='overfitting')\n",
    "        plt.plot(test_x.data.numpy(), test_pred_drop.data.numpy(), 'b--', lw=3, label='dropout(50%)')\n",
    "        plt.text(0, -1.2, 'overfitting loss=%.4f' % loss_func(test_pred_ofit, test_y).data.numpy(), fontdict={'size': 20, 'color':  'red'})\n",
    "        plt.text(0, -1.5, 'dropout loss=%.4f' % loss_func(test_pred_drop, test_y).data.numpy(), fontdict={'size': 20, 'color': 'blue'})\n",
    "        plt.legend(loc='upper left'); plt.ylim((-2.5, 2.5));plt.pause(0.1)\n",
    "\n",
    "        # 训练的时候再将屏蔽掉的dropout函数还原，使两个网络参数不同。用于训练\n",
    "        net_overfitting.train()\n",
    "        net_dropped.train()\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
